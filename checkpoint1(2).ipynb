{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1: Neural networks and deep learning\n",
    "---\n",
    "*Responsible:* Guillermo Hamity (<ghamity@ed.ac.uk>)\n",
    "\n",
    "In this checkpoint exercise, we will use neural networks to predict the **type** of weather *given* the available ground observations. You will be using observation data from **June 2019** across all UK Met Office weather stations.\n",
    "\n",
    "### Notes on the Dataset\n",
    "* You will be using weather observation data from the UK Met Office Datapoint service\n",
    "* Ground observations are made hourly at weather stations across the length of the UK \n",
    "* The data sample covers data from June 2019\n",
    "* Data collections for each day starts at 6.30pm. All observation data is listed in one day blocks\n",
    "* The time value column refers to the number of minutes after midnight \n",
    "* `Null` values for some features are expected (e.g. Wind Gust)\n",
    "* Data import and preparation is already provided \n",
    "\n",
    "\n",
    "This week, I am not providing example notebooks like `lecture2.ipynb` and `data-science-tools.ipynb` for Unit 2, though these may still be useful to you. Instead, I am **providing the imports for all of the modules and classes that you should need.** Think of these as LEGO blocks; you have the ones you need but may look up how to \"assemble\" them.\n",
    "\n",
    "### Notes on assessment\n",
    "* Try and calculate the answers to the exercises provided. If you are unable to complete the question, describe which approach you _would_ have taken to solve the problem\n",
    "* Code must be understandable and reproducible. Before grading the notebook kernel **may** be restarted and re-run, so make sure that your code can run from start to finish without any (unintentional) errors\n",
    "* If you are unsure on how to proceed please **ask one of the TAs** during the workshop\n",
    "- Notebooks should be submitted by **10am on Friday 9 October 2021** \n",
    "- This CP exercise sheet is divided into **6 sections**, corresponding to parts of the lecture, giving a maximum of **10 marks** in total:\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Exercise nos. | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 1. Conceptual questions               | <p align='left'>  1–5  | <p align='left'> 2.5 |\n",
    "| <p align='left'> 2. Data preprocessing and RandomForest                | <p align='left'>  6–9  | <p align='left'> 2.5 |\n",
    "| <p align='left'> 3. Neural networks in `scikit-learn`  | <p align='left'>  10–11 | <p align='left'> 1.5 | \n",
    "| <p align='left'> 4. Neural networks in `Keras`         | <p align='left'> 12–13 | <p align='left'> 2 |\n",
    "| <p align='left'> 5. Regularisation                     | <p align='left'> 14–15 | <p align='left'> 1.5 |\n",
    "| <p align='left'> 6. Bonus: Hyperparameter optimisation | <p align='left'> 16 | <p align='left'> 1.0 (\\*bonus\\*) |\n",
    "| <p align='left'> **Total** | | <p align='left'> **10 + 1** |\n",
    "\n",
    "- The total number of marks allocated for this CP is 10,\n",
    "    - 1 additional mark can be given (maximimally up to 10 marks in total) for \"bonus\" exercise on hyperparameter optimisation. If you are pressed for time, focus on the first five sections; those are the core ones.\n",
    "    - Half marks may be deducted for code legibility (i.e. very difficult to tell what you are doing), or for badly formated plots (i.e. no legends, axis labels etc.). The TAs will use their discression for this so comment code when applicable and keep relevant information in your plots.\n",
    "\n",
    "_Note:_ You can suppress double-printing of plots from the `plot` module by either _(a)_ adding a semicolon after the function call (_i.e._ `plot.<method>(...);`), or _(b)_ by capturing the return `pyplot.Figure` object as a variable (_i.e._ `fig = plot.<method>(...)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard import(s)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress unnecessary ConvergenceWarnings and DeprecationWarnings\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# Set a random seed variable to make workbook reproducible\n",
    "seed=5\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# Switch off multi-threading for TensorFlow\n",
    "from tensorflow.python.keras import backend as K\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                                  inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationID</th>\n",
       "      <th>StationName</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gust</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>WindDirection</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>PressureTrend</th>\n",
       "      <th>DewPoint</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.1</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>F</td>\n",
       "      <td>11.6</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.9</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>F</td>\n",
       "      <td>11.8</td>\n",
       "      <td>81.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>F</td>\n",
       "      <td>11.6</td>\n",
       "      <td>85.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.9</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>R</td>\n",
       "      <td>11.0</td>\n",
       "      <td>88.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>R</td>\n",
       "      <td>10.9</td>\n",
       "      <td>92.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StationID StationName  Elevation  Latitude  Longitude        Date  Time  \\\n",
       "0       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1020   \n",
       "1       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1080   \n",
       "2       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1140   \n",
       "3       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1200   \n",
       "4       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1260   \n",
       "\n",
       "   Gust  Temperature  Visibility WindDirection  WindSpeed  Pressure  \\\n",
       "0   NaN         16.1     30000.0             E        8.0    1019.0   \n",
       "1   NaN         14.9     22000.0             E        8.0    1019.0   \n",
       "2   NaN         14.0     14000.0             E        6.0    1018.0   \n",
       "3   NaN         12.9     12000.0           ENE        2.0    1019.0   \n",
       "4   NaN         12.0      9000.0             E        2.0    1019.0   \n",
       "\n",
       "  PressureTrend  DewPoint  Humidity  Type  \n",
       "0             F      11.6      74.5     0  \n",
       "1             F      11.8      81.5     0  \n",
       "2             F      11.6      85.4     0  \n",
       "3             R      11.0      88.1     0  \n",
       "4             R      10.9      92.9     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the prepared weather data\n",
    "obs = pd.read_csv('weather.csv')\n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106553, 17)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gust</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>DewPoint</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>106553.000000</td>\n",
       "      <td>106553.000000</td>\n",
       "      <td>106553.000000</td>\n",
       "      <td>106553.000000</td>\n",
       "      <td>106553.000000</td>\n",
       "      <td>7703.000000</td>\n",
       "      <td>106442.000000</td>\n",
       "      <td>92662.000000</td>\n",
       "      <td>102060.000000</td>\n",
       "      <td>99530.000000</td>\n",
       "      <td>106402.000000</td>\n",
       "      <td>106397.000000</td>\n",
       "      <td>106553.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6147.845636</td>\n",
       "      <td>114.466594</td>\n",
       "      <td>53.673022</td>\n",
       "      <td>-2.829034</td>\n",
       "      <td>702.914418</td>\n",
       "      <td>33.043749</td>\n",
       "      <td>14.958912</td>\n",
       "      <td>25698.164404</td>\n",
       "      <td>8.999510</td>\n",
       "      <td>1018.748337</td>\n",
       "      <td>10.348008</td>\n",
       "      <td>76.185240</td>\n",
       "      <td>0.99603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15821.503845</td>\n",
       "      <td>171.669120</td>\n",
       "      <td>2.466079</td>\n",
       "      <td>2.269594</td>\n",
       "      <td>412.057262</td>\n",
       "      <td>13.424855</td>\n",
       "      <td>4.294516</td>\n",
       "      <td>14263.873943</td>\n",
       "      <td>6.087882</td>\n",
       "      <td>6.327468</td>\n",
       "      <td>3.120565</td>\n",
       "      <td>17.208653</td>\n",
       "      <td>0.92971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3002.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>49.207900</td>\n",
       "      <td>-10.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>-28.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3204.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>51.565000</td>\n",
       "      <td>-4.149000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1015.000000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>65.300000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3414.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>53.175000</td>\n",
       "      <td>-2.663000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>79.200000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3769.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>55.285000</td>\n",
       "      <td>-1.097000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99142.000000</td>\n",
       "      <td>1245.000000</td>\n",
       "      <td>60.749000</td>\n",
       "      <td>1.348000</td>\n",
       "      <td>1380.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1036.000000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           StationID      Elevation       Latitude      Longitude  \\\n",
       "count  106553.000000  106553.000000  106553.000000  106553.000000   \n",
       "mean     6147.845636     114.466594      53.673022      -2.829034   \n",
       "std     15821.503845     171.669120       2.466079       2.269594   \n",
       "min      3002.000000       2.000000      49.207900     -10.250000   \n",
       "25%      3204.000000      20.000000      51.565000      -4.149000   \n",
       "50%      3414.000000      65.000000      53.175000      -2.663000   \n",
       "75%      3769.000000     132.000000      55.285000      -1.097000   \n",
       "max     99142.000000    1245.000000      60.749000       1.348000   \n",
       "\n",
       "                Time         Gust    Temperature    Visibility      WindSpeed  \\\n",
       "count  106553.000000  7703.000000  106442.000000  92662.000000  102060.000000   \n",
       "mean      702.914418    33.043749      14.958912  25698.164404       8.999510   \n",
       "std       412.057262    13.424855       4.294516  14263.873943       6.087882   \n",
       "min         0.000000     0.000000      -1.200000     20.000000       0.000000   \n",
       "25%       360.000000    29.000000      12.000000  14000.000000       5.000000   \n",
       "50%       720.000000    32.000000      14.500000  25000.000000       8.000000   \n",
       "75%      1020.000000    39.000000      17.500000  35000.000000      11.000000   \n",
       "max      1380.000000   105.000000      31.600000  75000.000000      81.000000   \n",
       "\n",
       "           Pressure       DewPoint       Humidity          Type  \n",
       "count  99530.000000  106402.000000  106397.000000  106553.00000  \n",
       "mean    1018.748337      10.348008      76.185240       0.99603  \n",
       "std        6.327468       3.120565      17.208653       0.92971  \n",
       "min      976.000000     -28.200000       0.800000       0.00000  \n",
       "25%     1015.000000       8.400000      65.300000       0.00000  \n",
       "50%     1018.000000      10.500000      79.200000       1.00000  \n",
       "75%     1024.000000      12.500000      90.000000       1.00000  \n",
       "max     1036.000000      22.700000     100.000000       3.00000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will use **8** input features (provided) and clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 8 input feature variables, 1 target variable data, and names of the 3 weather types\n",
    "features = ['Latitude', 'Elevation', 'Temperature', 'Visibility', 'WindSpeed', 'Pressure', 'Humidity', 'WindDirection']\n",
    "output   = ['Type']\n",
    "wtype    = ['Clear', 'Cloudy', 'Precip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define derived dataset containing only the relevant columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>WindDirection</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>74.5</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>81.5</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>85.4</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>88.1</td>\n",
       "      <td>ENE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>92.9</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>58.954</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>58.954</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>88.2</td>\n",
       "      <td>ESE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>58.954</td>\n",
       "      <td>26.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>87.2</td>\n",
       "      <td>ESE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>57.358</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>75.9</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>57.358</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>76.7</td>\n",
       "      <td>NNW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Latitude  Elevation  Temperature  Visibility  WindSpeed  Pressure  \\\n",
       "0      60.749       15.0         16.1     30000.0        8.0    1019.0   \n",
       "1      60.749       15.0         14.9     22000.0        8.0    1019.0   \n",
       "2      60.749       15.0         14.0     14000.0        6.0    1018.0   \n",
       "3      60.749       15.0         12.9     12000.0        2.0    1019.0   \n",
       "4      60.749       15.0         12.0      9000.0        2.0    1019.0   \n",
       "..        ...        ...          ...         ...        ...       ...   \n",
       "122    58.954       26.0         14.8      9000.0        6.0    1017.0   \n",
       "123    58.954       26.0         14.0     12000.0        6.0    1017.0   \n",
       "124    58.954       26.0         16.0     23000.0        5.0    1017.0   \n",
       "125    57.358        4.0         18.8     14000.0        9.0    1015.0   \n",
       "126    57.358        4.0         17.2     12000.0        9.0    1015.0   \n",
       "\n",
       "     Humidity WindDirection  Type  \n",
       "0        74.5             E     0  \n",
       "1        81.5             E     0  \n",
       "2        85.4             E     0  \n",
       "3        88.1           ENE     0  \n",
       "4        92.9             E     1  \n",
       "..        ...           ...   ...  \n",
       "122      86.6             E     1  \n",
       "123      88.2           ESE     1  \n",
       "124      87.2           ESE     1  \n",
       "125      75.9             N     0  \n",
       "126      76.7           NNW     1  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce to feature and type columns\n",
    "dataset = obs[features + output]\n",
    "\n",
    "# Drop duplicates and null values \n",
    "dataset = dataset.drop_duplicates().dropna()\n",
    "\n",
    "# Drop unrecorded weather type\n",
    "dataset = dataset[dataset.Type != 3]\n",
    "\n",
    "# Check shape \n",
    "dataset.shape\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conceptual questions (2.5 Marks)\n",
    "---\n",
    "This section covers **5** exercises on conceptual understanding of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Which are the most used activation functions and why do we (typically) need non-linear activation functions in neural networks? (0.5 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most used activation functions are:\n",
    "    \n",
    "    Sigmoid Activation, Relu, Softmax and tanh\n",
    "\n",
    "We typically need non-linear activation functions in neural networks, as without them hidden layers become useless as a composition of linear functions is just another linear function. Non-linear activation functions also allow for backpropagation (or gradient descent) while linear functions do not, as the derivative of a linear function has no relation to the input so it useless when trying to decide which weights produce a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Why do we need deep neural networks and which are the main differences between deep and shallow learning? (0.5 Mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deep Learning** is when a neural network has at least 2 or more hidden layers, and so can exploit high dimensional data to perform feature extraction and deep combination simultaneously to get an output\n",
    "- **Shallow Learning** has less than 2 hidden layers, and generally requires manual engineering and simple algorithms to get an output.\n",
    "\n",
    "\n",
    "Deep neural networks allow for increases in performance that scale with the amount of data supplied much better than older learning algorithms, and give us the ability to exploit high-dimensional data rather than having us manually break it down into low-dimensional data. This can allow the DL-network to find relevant features that wouldn't be obvious to the user who is breaking down the high-dimensional data.\n",
    "\n",
    "*Eg: The most important features for identifying cats can be determined by a human, and then plugged into a shallow combination system for an output. While in deep learning the high-dimensional 'cat' data can be plugged in directly and important features can be determined by the network itself and used to determine an output.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Discuss the Bias-variance trade-off and its relation to underfitting and overfitting of a model. Which are the charactheristics of an ideal model?  (0.5 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bias-variance trade-off is the relationship between these two factors (bias and variance) and how they correspond to either good or bad models.\n",
    "\n",
    "- Bias is a trainable parameter that shifts the activation function within a NN\n",
    "- Variance is the variability in your model's prediction, or how much your results can shift based on the given dataset\n",
    "\n",
    "An underfit model is one with high bias and low variance (so the model is inflexible and can't learn the relevant structure in the dataset). The high bias is attributed to poor performance on training data.\n",
    "\n",
    "An overfit model is one with low bias and high variance (so the model is too flexible and can learn random structures that don't represent the dataset as a whole). Overfit models show high loss on testing datasets but low loss on training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Given a neural network with 4 input nodes, 2 layers with 5 nodes each, and 1 output node, what is the total number of free (trainable) parameters in the network? Does it matter which activation function(s) are used?  (0.5 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 input nodes, 2 hidden layers of 5, and one 1 output mode.\n",
    "\n",
    "5x4 + 5x5 + 5x1 + 5 + 5 + 1 = 61 total trainable parameters \n",
    "\n",
    "*__But this includes the input and output, is this correct?__*\n",
    "\n",
    "Different activation functions are suitable for specific problems and allow for better training on specific problems when compared to their counterparts. ReLU for example is appropriate for regressing non-negative energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What are appropriate choice for _(a)_ the number of output nodes and _(b)_ output activation function(s) for each of the following tasks, and why? (0.5 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regression of the $x$, $y$, and $z$ coordinates of a single particle in an arbitrary coordinate system\n",
    "2. Regression of particle energy of a single particle\n",
    "3. Classification of two processes (signal vs. background)\n",
    "4. Classification among *N* classes (dog vs. cat vs. fish vs. ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\n",
    "\n",
    "1. 3 output nodes, as we have x,y and z regression.\n",
    "2. 1 output node, as we only have the particle energy.\n",
    "3. 2 output nodes, for both signal and background (although I guess you could technically have one as long as p1+p2 = 1)\n",
    "4. N output nodes (probability for each different class. dog, cat, fish, etc.)\n",
    "\n",
    "b)\n",
    "\n",
    "1. Linear, as the data involves regessing (but possibly negative) quantities. *__(ASK ABOUT THIS)__*\n",
    "2. ReLU, as the data is involves regressing non-negative quantities.\n",
    "3. Sigmoid, as this can be described as a binary classification probability (noise - 0, signal - 1).\n",
    "4. Softmax, as this can be described as a multiclass classification probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing and RTs (2.5 mark)\n",
    "---\n",
    "This section covers **4** exercises on data preparation, feature standardisation, and dataset splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics # Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import preprocessing # Import preprocessing for String-Int conversion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**_Comment on target format and one-hot encoding:_** By default, the target column (`Type`) contains one integer (0, 1, or 2) for each example, the integer specifying one of three possible types of weather. However, for doing multi-class classification (which this is), we want our neural network to have one output node per class (_i.e._ 3 output nodes in this case), such that the activation of each output node is interpreted as the likelihood for a given sample being of the type in question. Therefore, the target should also be a 3-element vector for each sample; this vector should be all zeros, except for a $1$ at the index corresponding to the type in question. This is called **one-hot encoding**, and a few examples are shown below:\n",
    "\n",
    "- type = 0 $\\to$ one-hot = $[1, 0, 0]$ for 3 classes\n",
    "- type = 1 $\\to$ one-hot = $[0, 1, 0]$ for 3 classes\n",
    "- type = 2 $\\to$ one-hot = $[0, 0, 1]$ for 3 classes\n",
    "\n",
    "This is the target towards which a neural network classifier is trained: That is, ideally, for an example of type 0, the network will output a large activation ($\\approx 1$) on the first output node (interpreted as a large likelihood for the first weather type), and very small activations ($\\ll 1$) on the two other output nodes (intepreted as small likelihoods for the two other weather types); and so on.\n",
    "\n",
    "The same type of one-hot encoding can be performed for any number of target classes $N_{c}$, which just results in $N_{c}$-element target vectors with a single non-zero entry each.\n",
    "\n",
    "To be user friendly, however, `scikit-learn` allows us to use integer targets for multi-class classification — it does the one-hot encoding for us \"under the hood.\" Similarly, `keras`, _can_ also allow us to use integer targets for multi-class classification, provided we use the appropropriate loss (`sparse_categorical_crossentropy`). Otherwise (if we use `categorical_crossentropy` loss), it expects one-hot encoded targets. Which approach you choose is up to you — but now you know what goes on.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prepare the feature and target arrays (0.5 mark)\n",
    "- Randomly sample **3,500** observations per weather type (**10,500** observations in total) from `dataset` into a new `pandas.DataFrame`; call it `sample`.\n",
    "- One-hot encode the **wind direction** variable (_i.e._ $N$ to $[1, 0, \\ldots, 0]$, $NNE$ to $[0, 1, \\ldots, 0]$, _etc._ ), to allow us to input it to the neural network. The exact order of the encoding (_i.e._ which direction corresponds to which index) doesn't matter. *Hint:*\n",
    "  - *Either:* Use the scikit-learn `ColumnTransformer` with the `OneHotEncoder` applied to the `WindDirection` column, and let the remainder of the features pass through un-transformed.\n",
    "  - *Or:* Use the `OneHotEncoder` class directly on the `WindDirection` column (use `sparse=False` in the `OneHotEncoder` constructor), and then concatenate with a `numpy.array` containing the remaining features.\n",
    "- Define `numpy.arrays` named `X` and `y` containing the training features (the 7 unmodified ones plus the one-hot encoded wind directions) and target, respectively.\n",
    "- Argue whether the shapes of `X` and `y` are as expected/as they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10500, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.sample(frac = 1)\n",
    "\n",
    "# Setting up samples for each dataset type\n",
    "sample0 = dataset[dataset['Type'] == 0].head(3500)\n",
    "sample1 = dataset[dataset['Type'] == 1].head(3500)\n",
    "sample2 = dataset[dataset['Type'] == 2].head(3500)\n",
    "\n",
    "sample = pd.concat([sample0,sample1,sample2])\n",
    "sample.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Train a Random Forest and evaluate the performance (1 mark)\n",
    "\n",
    "Decision trees work well with a mixture of features (of different scales, and bot binary and continuos), so we will train a random forest to do the job of categorisation.\n",
    "\n",
    "You are given the train test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7350, 23) (3150, 23)\n"
     ]
    }
   ],
   "source": [
    "#Import random fosets and confusion matrix metric\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# split dataset into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a RandomForestClassifier with 1000 estimators, `gini` seperation criteria, and max depth 4.\n",
    "2. Check the overal accuaracy on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the confusion_matrix method to return the confusion matrix normalised over the true lables, i.e. sum over rows should sum to 100%. Use the given colormap to plot the confusion matrix in a heatmap.\n",
    "    - Define the axis tick names to represent Clear, Cloudy or Precip\n",
    "    - Use suitable x and y axis labels\n",
    "    \n",
    "4. What are the true positive rates for clear, cloudy and perp? What is the probability that rain is forcast given that the day is clear?\n",
    "5. Which features does the random forest deem the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given plotting example for feature importance\n",
    "#plt.barh(range(23), rf.feature_importances_)\n",
    "#plt.yticks(range(23), list(range(16))+features[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Standardise the relevant features (0.5 mark)\n",
    "\n",
    "_Note:_ You shouldn't standardise the one-hot encoded wind directions; they already have the desired format. Perform a sanity check to make sure that the resulting features have the expected distributional properties (mean and standard deviation; or minimum and maximum value).\n",
    "- Hint:\n",
    "\n",
    "    - Use the scikit-learn `StandardScaler`\n",
    "    - Or use the scikit-learn `MinMaxScaler`\n",
    "\n",
    "- Perform a sanity check to make sure that the resulting features have the expected distributional properties (mean and standard deviation; or minimum and maximum value).\n",
    "    - The number of columns should match, and depending on the choice of standardisation, the last 7 columns should either have:\n",
    "      - (Using `StandardScaler`) means = 0 and standard deviations = 1; or\n",
    "      - (Using `MinMaxScaler`) min = 0, max = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Split the dataset into a training and a testing part (0.5)\n",
    "\n",
    "Reserve **30%** of data for testing. Check whether the resulting arrays have the expected shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural networks in `scikit-learn` (1.5 mark)\n",
    "---\n",
    "This section covers **2** exercises on constructing and training neural networks using the `scikit-learn` library, as well as evaluating neural network performance. `scikit-learn` provide many, very easy to use ML algorithms, including neural networks. These are called `MLPClassifier` (MLP = multi-layer perceptron; a historic name for densely connected, feed-forward neural networks) when used for classification, and `MLPRegressor` when used for regression. We will focus on the former for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Construct and train a neural network  (1 mark)\n",
    "\n",
    "- Create an `MLPClassifier` which\n",
    "    - has **1 hidden layer of 50 neurons** \n",
    "    - has **no regularization term**\n",
    "    - trains for a maximum of **100 epochs** \n",
    "    - uses a batch size of **32**\n",
    "- Fit the classifier using the standard `.fit()` member method.\n",
    "- Plot the loss function value as a function of number of epochs (0.5 of mark).\n",
    "  You can access the loss history through the `.loss_curve_` attribute of the `MLPClassifier` instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Performance evaluation (0.5 mark)\n",
    "\n",
    "- Using the testing dataset: \n",
    "    - Compute the overall accuracy for the classifier using the `MLPClassifier`'s `.score()` member method for both testing and training datasets.\n",
    "    - Compute the confusion matrix (normalised in true labels), and plot it \n",
    "- Discuss the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural networks in `Keras` (2 marks)\n",
    "---\n",
    "This section covers **2** exercises on constructing and training neural networks using the `Keras` library. `scikit-learn` is very easy to use, but libraries like `Keras` provide a lot more flexibility, which is why we will be using these extensively in the last two units of the _'Data science tools and machine learning'_ track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Construct a neural network in `Keras` (1 mark)\n",
    "\n",
    "- Create a `keras.Model` using the **Keras functional API**. The network should have:\n",
    "    - An input layer with the same number of nodes as the number of features in `X`.\n",
    "    - A single, densely connected hidden layer with **50 nodes** equipped with **ReLU activation**.\n",
    "    - A densely connected output layer with **3 nodes** (the number of types of weather we're classifying) equipped with **softmax activation**.\n",
    "- Compile the model the using the **Adam optimiser**, add `'accuracy'` as metric, and use either:\n",
    "    - `categorical_crossentropy` loss, if you have one-hot encoded the targets `y`, or\n",
    "    - `sparse_categorical_crossentropy` loss if you hare using integer-valued targets.\n",
    "- Use the `.summary()` member method to print an overview of the model you have created, explain the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Train a `Keras` neural network (1 mark)\n",
    "\n",
    "- Use the `.fit()` member method to train the network on the **training dataset** for **100 epochs** with a **batch size of 32**. Use **20% of the data for validation** and make sure to have `Keras` **shuffle** the training data between epochs. Save the fit history by doing `history_mld = .....`\n",
    "- Print the classification accuracy using the `.evaluate()` member method, for both the training and testing dataset. Comment on the results.\n",
    "- Plot val_loss and loss functions from the fit history. On the same plot, plot the sklearn curve from the excercise above. Note the sklearn NN does not provide a complementary validation loss history, so only plot the training loss.\n",
    "- Comment on the results of the overall accuracy compared to the scikit-learn method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regularisation (1.5 marks)\n",
    "---\n",
    "This section covers **2** exercises on the impact of weight regularisaton. Note that $L_{1}$- and $L_{2}$-regularisation may also be applied to the activation of intermediate layers. Also, a similar regularising effect could be achieved using **dropout** regularisation, which you are encouraged to try out, but which we won't study in this CP exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from tensorflow.python.keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Define `Keras` model factory method (0.5 mark)\n",
    "\n",
    "- Define a python function called `big_model_fn` which takes the followng three arguments:\n",
    "    - `l1`: A float specifying the $L_{1}$ regularisation factor (default value: 0)\n",
    "    - `l2`: A float specifying the $L_{2}$ regularisation factor (default value: 0)\n",
    "    - `name`: A string, specifying the name of the model (default value: None)\n",
    "- Indside the function, you should:\n",
    "    - Construct a `Keras` model using the functional API, which has:\n",
    "        - An input layer with the same number of nodes as the number of features in `X`.\n",
    "        - **Two** densely connected hidden layer with **100 nodes** each, both equipped with **ReLU activation**.\n",
    "        - Both hidden layers should be subject to kernel regularisation (_i.e._ weight regularisation) with the regularisation factors specified as an input.\n",
    "        - A densely connected output layer with **3 nodes** (the number of types of weather we're classifying) equipped with **softmax activation**.\n",
    "        - A name given by the corresponding argument.\n",
    "    - Compile the model in the same way as in **Exercise 14.**\n",
    "- The function should return the compiled `Keras` model. \n",
    "\n",
    "The method will provide a convenient way of constructing and compiling a number of \"big\"/deep `Keras` models which differ only by their regularisation and name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Train \"big\" models with and without regularisation (1 mark)\n",
    "\n",
    "- Construct three \"big\" model using the factory method:\n",
    "     - One with default parameters\n",
    "     - One with `l1=0.0003` and  `name='Big model (L1-regularised)'`\n",
    "     - One with `l2=0.003`  and `name='Big model (L2-regularised)'`\n",
    "- Train each one as in **Exercise 15.**\n",
    "- Compare first the loss history of the un-regularised \"big\" model to that of the small model from **Exercise 15** using the `plot.loss()` method.\n",
    "- Then, compare the loss histories of all three \"big\" models with that of the small model.\n",
    "- Plot the loss and val loss of all 4 models and discuss the results. Target these points:\n",
    "    - Compare the performance of deep vs shallow models on the testing sets\n",
    "    - Compare the level of ovetraining (training vs testing loss)\n",
    "    - Note: Don't be alarmed if the shallow network performs slightly better that the deeper ones, this is dataset dependant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bonus: Hyperparameter optimisation (1\\*bonus\\* mark)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers **1** exercise on the on hyperparameter optimisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_**Comment on simplified hyperparameter optimisation example:**_ You will try to perform a simple optimisation using a grid search\n",
    "\n",
    "For convenience, we will be using the `scikit-learn` `MLPClassifier` as our base class, but the same principles apply to just about any ML model constructed in any framework. Just as in the examples in the lecture, we will restrict the hyperparameter space to just two dimensions:\n",
    "\n",
    "* the number of hidden layers, `nb_layers`, and\n",
    "* the number of nodes per hidden layer, `nb_nodes_per_layer`, which is taken to be the same for all hidden layers for simplicity.\n",
    "\n",
    "Since the `scikit-learn` neural network classifier class doesn't support these two hyperparameters by default, provided is a simple wrapper class, that works exactly like `MLPClassifier`, it just takes the two parameters above as arguments in the constructor. Don't worry about understanding it in detail. This allows us to call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifierWrapper(MLPClassifier):\n",
    "    \"\"\"\n",
    "    Wrapper around `sklearn.neural_network.MLPClassifier` with a convenient set \n",
    "    of properties (nb_layers and nb_nodes_per_layer) suitable for hyperparameter \n",
    "    optimisation exercises.\n",
    "    \n",
    "    Arguments:\n",
    "        nb_layers: Integer, number of hidden layers\n",
    "        nb_nodes_per_layer: Number of nodes per hidden layer, taken to be the \n",
    "            same for all for convenience.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self, nb_layers=1, nb_nodes_per_layer=100, **kwargs):\n",
    "        \n",
    "        # Member variables\n",
    "        self._nb_layers = nb_layers\n",
    "        self._nb_nodes_per_layer = nb_nodes_per_layer  \n",
    "        \n",
    "        # Call base class (`MLPClassifier`) constructor\n",
    "        super(MLPClassifierWrapper, self).__init__(**kwargs)\n",
    "        \n",
    "        # Trigger `_set_architecture`\n",
    "        self._set_architecture()\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def nb_layers(self):\n",
    "        return self._nb_layers\n",
    "    \n",
    "    @property\n",
    "    def nb_nodes_per_layer(self):\n",
    "        return self._nb_nodes_per_layer \n",
    "\n",
    "    @nb_layers.setter\n",
    "    def nb_layers(self, value):\n",
    "        self._nb_layers = value\n",
    "        self._set_architecture()\n",
    "        return\n",
    "    \n",
    "    @nb_nodes_per_layer.setter\n",
    "    def nb_nodes_per_layer(self, value):\n",
    "        self._nb_nodes_per_layer = value\n",
    "        self._set_architecture()\n",
    "        return\n",
    "    \n",
    "    def _set_architecture (self):\n",
    "        \"\"\"\n",
    "        Sets the `hidden_layer_sizes` parameter of the base `MLPClassifier` \n",
    "        class, based on the two custom parameters we have chosen.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hidden_layer_sizes = tuple([self._nb_nodes_per_layer for _ in range(self._nb_layers)])\n",
    "        return\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Perform a grid search (1 mark)\n",
    "\n",
    "- Construct a python `dict` called `param_grid` which specifies the hyperparameter configurations to try for each parameter dimension. That is, it should have\n",
    "    - `\"nb_layers\"` and `\"nb_nodes_per_layer\"` as keys, and\n",
    "    - lists of integers as values, corresponding to the values of each parameter you want to try out (_e.g._ [1, 2, ...])\n",
    "- Choose a reasonable set of values for each parameter; about a handful for each.\n",
    "- Use the `GridSearchCV` class to perform _**3**_**-fold** cross validation (CV) optimisation of the validation **accuracy**\n",
    "    - Hint: You can use the `n_jobs=...` argument to enable multi-processing, thereby speeding up the optimisation, at the expense of reproducibility.\n",
    "- The base classifier should be an instance of `MLPClassifierWrapper` set to train for **100 epochs**.\n",
    "- Present the results:\n",
    "    - Print the best parameter configuration found. GridSearchCV has a public member which stores this. Read doc.\n",
    "    - Print the mean and standard deviation of the test scores for the best configuration found. (_Hint:_ These can be found in the `.cv_results_` attribute)\n",
    "    - Plot the optimisation results using the `plot.optimisation` method.\n",
    "- Discuss the results. What would happen if the best result is foundon the edge of the parameter grid?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
